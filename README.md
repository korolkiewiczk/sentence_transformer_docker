# Docker Image README

This README provides an overview of the Docker image defined in the given Dockerfile and explains what it does.

## Overview

The provided Docker image is designed to run a Python application that utilizes the [Hugging Face Transformers](https://huggingface.co/transformers) library, [Sentence Transformers](https://github.com/UKPLab/sentence-transformers), and [Flask](https://flask.palletsprojects.com/en/2.1.x/) to serve text embeddings using a pre-trained model. Specifically, it loads a Sentence Transformer model called `"sdadas/mmlw-retrieval-roberta-large"` and exposes endpoints to encode text and generate embeddings. 

**NOTE**: the sentence transformer model `"sdadas/mmlw-retrieval-roberta-large"` is dedicated for Polish language. Reference: [MMLW-retrieval-roberta-large](https://huggingface.co/sdadas/mmlw-retrieval-roberta-large)

## Dockerfile

Here is a breakdown of the Dockerfile:

```dockerfile
# Use the Python 3.10 slim base image
FROM python:3.10-slim

# Set the working directory inside the container
WORKDIR /app

# Install required Python packages: Transformers, PyTorch, Sentence Transformers, and Flask
RUN pip install transformers \
    && pip3 install torch --index-url https://download.pytorch.org/whl/cpu \
    && pip install sentence-transformers \
    && pip install Flask

# Copy the Python script 'main.py' into the working directory
COPY main.py /app/

# Load the Sentence Transformer model when building the image
RUN python -c "from sentence_transformers import SentenceTransformer; model = SentenceTransformer('sdadas/mmlw-retrieval-roberta-large')"

# Define the command to run when the container is started
CMD ["python", "/app/main.py"]
```

## Application (main.py)

The main Python script (`main.py`) inside the container defines a Flask application that serves text embeddings using the pre-trained Sentence Transformer model.

Here are the main functionalities provided by this application:

- **Encode Text**: It can take a text input and encode it into a numerical embedding using the pre-trained Sentence Transformer model.

- **Generate Embeddings**: It can accept multiple text inputs and generate embeddings for each of them.

- **Docker Container Configuration**: The application is configured to run on the host '0.0.0.0' and port 5000 when the Docker container is started.

## Usage

You can build and run the Docker image to start the Flask application. Once running, you can interact with the application using HTTP requests to the exposed endpoints:

- `POST /v1/embeddings`: Encode a text input and receive the corresponding embedding (similar to OpenAI API).

- `GET /generate-embedding`: Generate an embedding for a query provided as a URL parameter.

- `POST /generate-embeddings`: Generate embeddings for multiple texts provided in the request body.

## Example Usage

To use this Docker image and its associated Flask application, you can perform the following steps:

1. Build the Docker image using the provided Dockerfile (build.sh).

2. Run a Docker container from the built image (run.sh).

3. Interact with the Flask application by sending HTTP requests to the exposed endpoints (e.g., `/v1/embeddings`, `/generate-embedding`, `/generate-embeddings`).

4. Use the embeddings generated by the Sentence Transformer model for your specific application or analysis.

Please make sure to provide the necessary input data according to the expected request formats defined in the application endpoints.

For more details on how to use the endpoints and interact with the application, refer to the application code in `main.py`.

Also see `samples` folder for some examples how the api can be used.